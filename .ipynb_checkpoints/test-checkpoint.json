{
    "category": "subject",
    "title": "Artificial intelligence",
    "pageid": 1164,
    "page": {
        "title": "Artificial intelligence",
        "summary": "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence\u2014the ability to complete any task performable by a human on an at least equal level\u2014is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.",
        "links": {
            "notable people:": []
        },
        "sections": [
            {
                "title": "Goals",
                "text": "The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research."
            },
            {
                "title": "Techniques",
                "text": "AI research uses a wide variety of techniques to accomplish the goals above."
            },
            {
                "title": "Applications",
                "text": "AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO)."
            },
            {
                "title": "Ethics",
                "text": "AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning."
            },
            {
                "title": "History",
                "text": "The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCullouch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022."
            },
            {
                "title": "Philosophy",
                "text": "Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI."
            },
            {
                "title": "Future",
                "text": ""
            },
            {
                "title": "In fiction",
                "text": "Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence."
            },
            {
                "title": "See also",
                "text": "Artificial general intelligence\nArtificial intelligence content detection \u2013 Software to detect AI-generated content\nBehavior selection algorithm \u2013 Algorithm that selects actions for intelligent agents\nBusiness process automation \u2013 Automation of business processes\nCase-based reasoning \u2013 Process of solving new problems based on the solutions of similar past problems\nComputational intelligence \u2013 Ability of a computer to learn a specific task from data or experimental observation\nDigital immortality \u2013 Hypothetical concept of storing a personality in digital form\nEmergent algorithm \u2013 Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies \u2013 Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence \u2013 List of definitions of terms and concepts commonly used in the study of artificial intelligence\nIntelligence amplification \u2013 Use of information technology to augment human intelligence\nMind uploading \u2013 Hypothetical process of digitally emulating a brain\nMoravec's paradox \u2013 Observation that perception requires more computation than reasoning\nOrganoid intelligence \u2013 Use of brain cells and brain organoids for intelligent computing\nRobotic process automation \u2013 Form of business process automation technology\nWeak artificial intelligence \u2013 Form of artificial intelligence\nWetware computer \u2013 Computer composed of organic material\nHallucination (artificial intelligence) \u2013 Erroneous material generated by AI"
            },
            {
                "title": "Explanatory notes",
                "text": ""
            },
            {
                "title": "References",
                "text": ""
            },
            {
                "title": "Further reading",
                "text": ""
            },
            {
                "title": "External links",
                "text": "\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005)."
            }
        ]
    },
    "images": [
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/1/1b/AI_hierarchy.svg",
            "title": "File:AI hierarchy.svg",
            "caption": "AI hierarchy"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/e/e4/Artificial_neural_network.svg",
            "title": "File:Artificial neural network.svg",
            "caption": "Artificial neural network"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/8/87/Capek_play.jpg",
            "title": "File:Capek play.jpg",
            "caption": "Capek play"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/en/4/4a/Commons-logo.svg",
            "title": "File:Commons-logo.svg",
            "caption": "Commons-logo"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/6/64/Dall-e_3_%28jan_%2724%29_artificial_intelligence_icon.png",
            "title": "File:Dall-e 3 (jan '24) artificial intelligence icon.png",
            "caption": "Dall-e 3 (jan '24) artificial intelligence icon"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif",
            "title": "File:EM Clustering of Old Faithful data.gif",
            "caption": "EM Clustering of Old Faithful data"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/e/e8/General_Formal_Ontology.svg",
            "title": "File:General Formal Ontology.svg",
            "caption": "General Formal Ontology"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/4/41/Global_thinking.svg",
            "title": "File:Global thinking.svg",
            "caption": "Global thinking"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif",
            "title": "File:Gradient descent.gif",
            "caption": "Gradient descent"
        },
        {
            "url": "https://upload.wikimedia.org/wikipedia/commons/2/27/Kismet-IMG_6007-gradient.jpg",
            "title": "File:Kismet-IMG 6007-gradient.jpg",
            "caption": "Kismet-IMG 6007-gradient"
        }
    ],
    "infobox": {},
    "subreddits": {
        "CharacterAI": "https://www.reddit.com/r/CharacterAI",
        "singularity": "https://www.reddit.com/r/singularity",
        "artificial": "https://www.reddit.com/r/artificial",
        "aiArt": "https://www.reddit.com/r/aiArt",
        "ChatGPT": "https://www.reddit.com/r/ChatGPT"
    }
}